{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "import operator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "global checkinsStat, twitterLDAStat, twitterLIWCStat, twitterTextStat, instConceptsStst, train, test, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "def read_data():\n",
    "    global checkinsStat, twitterLDAStat, twitterLIWCStat, twitterTextStat, instConceptsStst, train, test, ground_truth\n",
    "    # Foursquare \n",
    "    checkinsStat = pd.read_csv('featuresLondon/Foursquare/venueCategoriesFeatures5Months.csv')\n",
    "    #checkins = pd.read_csv('featuresNewYork/Foursquare/checkinsSingapore.csv', error_bad_lines=False)\n",
    "\n",
    "    # Twitter\n",
    "    twitterLDAStat = pd.read_csv('featuresLondon/Twitter/LDA50Features.csv')\n",
    "    twitterLIWCStat = pd.read_csv('featuresLondon/Twitter/LIWCFeatures.csv')\n",
    "    twitterTextStat = pd.read_csv('featuresLondon/Twitter/manuallyDefinedTextFeatures.csv')\n",
    "\n",
    "    # TODO twits = ??\n",
    "\n",
    "    # Instagram\n",
    "    instConceptsStst = pd.read_csv('featuresLondon/Instagram/imageConceptsFeatures.csv')\n",
    "\n",
    "    # Ground truth\n",
    "    train = pd.read_csv('LondonGroundTruth.csv', encoding = \"ISO-8859-1\")\n",
    "    train.rename(columns = {'row ID' : '_id'}, inplace=True)\n",
    "    train = train.dropna(subset=['relationship'])\n",
    "\n",
    "\n",
    "#     test = pd.read_csv('NYTest.csv')\n",
    "#     test.rename(columns = {'row ID' : '_id'}, inplace=True)\n",
    "#     test = test.dropna(subset=['relationship'])\n",
    "\n",
    "\n",
    "#     ground_truth = train.append(test, ignore_index = True)\n",
    "    ground_truth = train\n",
    "    ground_truth['rand'] = np.random.random_sample((len(ground_truth['_id'],)))\n",
    "    ground_truth = ground_truth[['_id', 'relationship', 'rand']]\n",
    "    ground_truth.relationship[ground_truth['relationship'] != 'single'] = int(0)\n",
    "    ground_truth.relationship[ground_truth['relationship'] == 'single'] = int(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for checkins history\n",
    "read_data()\n",
    "checkinsStat = checkinsStat.merge(ground_truth, on = '_id')\n",
    "\n",
    "train = checkinsStat[checkinsStat['rand'] > 0.1]\n",
    "test = checkinsStat[checkinsStat['rand'] <= 0.1]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "test = SelectKBest(score_func = chi2, k = 100)\n",
    "fit = test.fit(X_train, Y_train.astype(int))\n",
    "X_train = fit.transform(X_train)\n",
    "X_test = fit.transform(X_test)\n",
    "\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Models for checking history\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for twitter statistics\n",
    "read_data()\n",
    "twitterStat = ground_truth.merge(twitterTextStat, on = '_id')\n",
    "twitterStat = twitterStat.merge(twitterLDAStat, on = '_id')\n",
    "train = twitterStat[twitterStat['rand'] > 0.2]\n",
    "test = twitterStat[twitterStat['rand'] <= 0.2]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "test = SelectKBest(score_func = f_classif, k = 30)\n",
    "fit = test.fit(X_train, Y_train.astype(int))\n",
    "X_train = fit.transform(X_train)\n",
    "X_test = fit.transform(X_test)\n",
    "\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for twitter history\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for instagram statistics\n",
    "read_data()\n",
    "instConceptsStst = ground_truth.merge(instConceptsStst, on = '_id')\n",
    "train = instConceptsStst[instConceptsStst['rand'] > 0.2]\n",
    "test = instConceptsStst[instConceptsStst['rand'] <= 0.2]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for instagram history\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Twitter + Foursware\n",
    "read_data()\n",
    "twitterStat = ground_truth.merge(twitterTextStat, on = '_id')\n",
    "twitterStat = twitterStat.merge(twitterLDAStat, on = '_id')\n",
    "TwitterFoursqare = twitterStat.merge(checkinsStat, on = '_id')\n",
    "\n",
    "train = TwitterFoursqare[TwitterFoursqare['rand'] > 0.1]\n",
    "test = TwitterFoursqare[TwitterFoursqare['rand'] <= 0.1]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "max_score = 0\n",
    "max_it = 0\n",
    "for i in range(50, 450):\n",
    "    test = SelectKBest(k = i)\n",
    "    fit = test.fit(X_train, Y_train.astype(int))\n",
    "    X_train_it = fit.transform(X_train)\n",
    "    X_test_it = fit.transform(X_test)\n",
    "\n",
    "    SVC = svm.SVC()\n",
    "    SVC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_svc = SVC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    GBC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_gbc = GBC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    LG = LogisticRegression()\n",
    "    LG.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_lg = LG.score(X_test_it, Y_test.astype(int))\n",
    "    \n",
    "    max_it_score = max(f1_svc, f1_gbc, f1_lg)\n",
    "    if max_it_score > max_score:\n",
    "        max_score = max_it_score\n",
    "        max_it = i\n",
    "        print(max_score)\n",
    "        \n",
    "print (\"Max Score: \" + str(max_score))\n",
    "print (\"KBest: \" + str(max_it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for Twitter + Foursquare\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_data()\n",
    "# Twitter + Instagram\n",
    "TwitterInstagram = twitterStat.merge(instConceptsStst, on = '_id')\n",
    "TwitterInstagram = ground_truth.merge(TwitterInstagram, on = '_id')\n",
    "\n",
    "\n",
    "train = TwitterInstagram[TwitterInstagram['rand_x'] > 0.1]\n",
    "test = TwitterInstagram[TwitterInstagram['rand_x'] <= 0.1]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship_x', 'rand_x', 'relationship_y', 'rand_y'], axis = 1)\n",
    "Y_train = train['relationship_x']\n",
    "\n",
    "X_test = test.drop(['_id','relationship_x', 'rand_x', 'relationship_y', 'rand_y'], axis = 1)\n",
    "Y_test = test['relationship_x']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "max_score = 0\n",
    "max_it = 0\n",
    "for i in range(50, 600):\n",
    "    test = SelectKBest(k = i)\n",
    "    fit = test.fit(X_train, Y_train.astype(int))\n",
    "    X_train_it = fit.transform(X_train)\n",
    "    X_test_it = fit.transform(X_test)\n",
    "\n",
    "    SVC = svm.SVC()\n",
    "    SVC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_svc = SVC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    GBC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_gbc = GBC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    LG = LogisticRegression()\n",
    "    LG.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_lg = LG.score(X_test_it, Y_test.astype(int))\n",
    "    \n",
    "    max_it_score = max(f1_svc, f1_gbc, f1_lg)\n",
    "    if max_it_score > max_score:\n",
    "        max_score = max_it_score\n",
    "        max_it = i\n",
    "        print(max_score)\n",
    "        \n",
    "print (\"Max Score: \" + str(max_score))\n",
    "print (\"KBest: \" + str(max_it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for Twitter + Instagram\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train.astype(int))\n",
    "print(SVC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train.astype(int))\n",
    "print(GBC.score(X_test, Y_test.astype(int)))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train.astype(int))\n",
    "print(LG.score(X_test, Y_test.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Foursquare + Instagram\n",
    "read_data()\n",
    "FoursquareInstagram = checkinsStat.merge(instConceptsStst, on = '_id')\n",
    "FoursquareInstagram = ground_truth.merge(FoursquareInstagram, on = '_id')\n",
    "\n",
    "train = FoursquareInstagram[FoursquareInstagram['rand'] > 0.2]\n",
    "test = FoursquareInstagram[FoursquareInstagram['rand'] <= 0.2]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "max_it = 0\n",
    "for i in range(50, 1200):\n",
    "    test = SelectKBest(k = i)\n",
    "    fit = test.fit(X_train, Y_train.astype(int))\n",
    "    X_train_it = fit.transform(X_train)\n",
    "    X_test_it = fit.transform(X_test)\n",
    "\n",
    "    SVC = svm.SVC()\n",
    "    SVC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_svc = SVC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    GBC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_gbc = GBC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    LG = LogisticRegression()\n",
    "    LG.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_lg = LG.score(X_test_it, Y_test.astype(int))\n",
    "    \n",
    "    max_it_score = max(f1_svc, f1_gbc, f1_lg)\n",
    "    if max_it_score > max_score:\n",
    "        max_score = max_it_score\n",
    "        max_it = i\n",
    "        print(\"Iteration: \" + str(i) + \" Score: \" + str(max_score))\n",
    "        \n",
    "print (\"Max Score: \" + str(max_score))\n",
    "print (\"KBest: \" + str(max_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for Foursquare + Instagram\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train)\n",
    "f1_svc = f1_score(Y_test, SVC.predict(X_test))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train)\n",
    "f1_gbc = f1_score(Y_test, GBC.predict(X_test))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train)\n",
    "f1_lg = f1_score(Y_test, LG.predict(X_test))\n",
    "\n",
    "print(\"SVC f score \" + str(f1_svc))\n",
    "print(\"GBC f score \" + str(f1_gbc))\n",
    "print(\"LG f score \" + str(f1_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Foursquare + Instagram + Twitter\n",
    "read_data()\n",
    "FoursquareInstagramTwitter = checkinsStat.merge(instConceptsStst, on = '_id')\n",
    "FoursquareInstagramTwitter = FoursquareInstagramTwitter.merge(twitterTextStat, on = '_id')\n",
    "FoursquareInstagramTwitter = ground_truth.merge(FoursquareInstagramTwitter, on = '_id')\n",
    "\n",
    "train = FoursquareInstagram[FoursquareInstagram['rand'] > 0.2]\n",
    "test = FoursquareInstagram[FoursquareInstagram['rand'] <= 0.2]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "max_it = 0\n",
    "for i in range(50, 1200):\n",
    "    test = SelectKBest(k = i)\n",
    "    fit = test.fit(X_train, Y_train.astype(int))\n",
    "    X_train_it = fit.transform(X_train)\n",
    "    X_test_it = fit.transform(X_test)\n",
    "\n",
    "    SVC = svm.SVC()\n",
    "    SVC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_svc = SVC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    GBC.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_gbc = GBC.score(X_test_it, Y_test.astype(int))\n",
    "\n",
    "    LG = LogisticRegression()\n",
    "    LG.fit(X_train_it, Y_train.astype(int))\n",
    "    f1_lg = LG.score(X_test_it, Y_test.astype(int))\n",
    "    \n",
    "    max_it_score = max(f1_svc, f1_gbc, f1_lg)\n",
    "    if max_it_score > max_score:\n",
    "        max_score = max_it_score\n",
    "        max_it = i\n",
    "        print(\"Iteration: \" + str(i) + \" Score: \" + str(max_score))\n",
    "        \n",
    "print (\"Max Score: \" + str(max_score))\n",
    "print (\"KBest: \" + str(max_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Models for Foursquare + Instagram + Twitter\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train)\n",
    "f1_svc = f1_score(Y_test, SVC.predict(X_test))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train)\n",
    "f1_gbc = f1_score(Y_test, GBC.predict(X_test))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train)\n",
    "f1_lg = f1_score(Y_test, LG.predict(X_test))\n",
    "\n",
    "print(\"SVC f score \" + str(f1_svc))\n",
    "print(\"GBC f score \" + str(f1_gbc))\n",
    "print(\"LG f score \" + str(f1_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectionMultiData = FoursquareInstagramTwitter[['_id', 'relationship', 'rand', 'numberOfImages', \n",
    "                                                 'numberOfTweets', 'categoryMentions']]\n",
    "\n",
    "train = selectionMultiData[selectionMultiData['rand'] > 0.2]\n",
    "test = selectionMultiData[selectionMultiData['rand'] <= 0.2]\n",
    "\n",
    "X_train = train.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_train = train['relationship']\n",
    "\n",
    "X_test = test.drop(['_id', 'relationship', 'rand'], axis = 1)\n",
    "Y_test = test['relationship']\n",
    "\n",
    "print (\"Train data size: \" + str(len(Y_train)))\n",
    "print (\"Test data size: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selection models for Foursquare + Instagram + Twitter\n",
    "SVC = svm.SVC()\n",
    "SVC.fit(X_train, Y_train)\n",
    "f1_svc = f1_score(Y_test, SVC.predict(X_test))\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X_train, Y_train)\n",
    "f1_gbc = f1_score(Y_test, GBC.predict(X_test))\n",
    "\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, Y_train)\n",
    "f1_lg = f1_score(Y_test, LG.predict(X_test))\n",
    "\n",
    "print(\"SVC f score \" + str(f1_svc))\n",
    "print(\"GBC f score \" + str(f1_gbc))\n",
    "print(\"LG f score \" + str(f1_lg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
